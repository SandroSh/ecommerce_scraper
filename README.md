# E-commerce Data Scraper & Analytics Platform

A comprehensive Python-based e-commerce data scraping and analysis platform that extracts product information from online stores and provides detailed analytics, trend analysis, and automated reporting.

## ğŸš€ Features

### **Multi-Source Data Collection**
- âœ… **Scrapy Framework**: Industrial-strength web scraping with zoommer.ge integration
- âœ… **API Integration**: Direct API access for efficient data collection
- âœ… **Rate Limiting**: Intelligent throttling and concurrent request management
- âœ… **Error Handling**: Robust retry logic and status code handling
- âœ… **Multiple Categories**: Phones, laptops, fridges, TVs support

### **Data Processing & Validation**
- âœ… **Data Cleaning Pipeline**: Automated validation and standardization
- âœ… **Quality Scoring**: 0-100 data quality assessment
- âœ… **Multi-format Export**: JSON, CSV, Excel output support
- âœ… **Deduplication**: Intelligent duplicate record removal
- âœ… **Field Extraction**: Automatic feature extraction (storage, RAM, etc.)

### **Advanced Analytics**
- âœ… **Statistical Analysis**: Comprehensive descriptive statistics
- âœ… **Trend Analysis**: Time-based patterns and forecasting
- âœ… **Price Analytics**: Distribution analysis and market insights
- âœ… **Brand Intelligence**: Market share and positioning analysis
- âœ… **Visualization**: Automated chart generation (matplotlib/seaborn)

### **Automated Reporting**
- âœ… **Executive Summaries**: Key insights and recommendations
- âœ… **Visual Reports**: PNG charts and graphs
- âœ… **Multiple Formats**: JSON, HTML, and text reports
- âœ… **Comparative Analysis**: Cross-category and time-period comparisons

## ğŸ“ Project Structure

```
ecommerce_scraper/
â”œâ”€â”€ main.py                    # Main scraping entry point
â”œâ”€â”€ run_spider.py             # Simple spider runner
â”œâ”€â”€ analyze_data.py           # Data analysis CLI
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ data_output/             # Output directory
â”‚   â”œâ”€â”€ raw/                 # Raw scraped data
â”‚   â”œâ”€â”€ processed/           # Cleaned and validated data
â”‚   â””â”€â”€ reports/             # Analysis reports and visualizations
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analysis/            # Analytics modules
â”‚   â”‚   â”œâ”€â”€ statistics.py    # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ trends.py        # Trend analysis
â”‚   â”‚   â””â”€â”€ reports.py       # Report generation
â”‚   â”œâ”€â”€ data/                # Data processing
â”‚   â”‚   â”œâ”€â”€ processors.py    # Data cleaning and validation
â”‚   â”‚   â”œâ”€â”€ models.py        # Data models
â”‚   â”‚   â””â”€â”€ database.py      # Database integration
â”‚   â”œâ”€â”€ scrapers/            # Scraping modules
â”‚   â”‚   â””â”€â”€ zoomer_scraper/  # Scrapy project for zoommer.ge
â”‚   â””â”€â”€ utils/               # Utility functions
â””â”€â”€ docs/                    # Documentation
```

## ğŸ› ï¸ Installation

### **Prerequisites**
- Python 3.8+
- pip package manager

### **Setup**
```bash
# Clone the repository
git clone <repository-url>
cd ecommerce_scraper

# Install dependencies
pip install -r requirements.txt

# Create output directories
mkdir -p data_output/{raw,processed,reports}
```

### **Key Dependencies**
- `scrapy` - Web scraping framework
- `pandas` - Data manipulation and analysis
- `matplotlib` - Data visualization
- `seaborn` - Statistical visualization
- `numpy` - Numerical computing
- `scipy` - Scientific computing

## ğŸš€ Quick Start

### **1. Basic Data Scraping**

**Scrape 10 phone products:**
```bash
python main.py --category phones --max_products 10
```

**Scrape 50 laptops:**
```bash
`python main.py --category laptops --max_products 50`
```

**Available categories:** `phones`, `laptops`, `fridges`, `tvs`

### **2. Data Analysis**

**Analyze scraped data:**
```bash
python analyze_data.py analyze --input data_output/raw/zoomer_phones_10.json
```

**Complete pipeline (scrape + process + analyze):**
```bash
# Step 1: Scrape data
python main.py --category phones --max_products 100

# Step 2: Run complete analysis
python analyze_data.py pipeline --input data_output/raw/zoomer_phones_100.json --output data_output/phone_analysis
```

## ğŸ“Š Complete Workflow Example

### **Scenario: Analyze 100 Phone Products**

```bash
# 1. Scrape 100 phone products from zoommer.ge
python main.py --category phones --max_products 100

# 2. Run complete analysis pipeline
python analyze_data.py pipeline --input data_output/raw/zoomer_phones_100.json --output data_output/phone_analysis_100

# 3. View results
ls data_output/phone_analysis_100/
# Output:
# - ecommerce_analysis_report_[timestamp].json  (Detailed JSON report)
# - ecommerce_analysis_report_[timestamp].html  (HTML dashboard)
# - price_distribution.png                      (Price analysis charts)
# - category_analysis.png                       (Category breakdown)
# - brand_analysis.png                          (Brand market share)
# - time_analysis.png                           (Time-based patterns)
# - report_summary_[timestamp].txt              (Executive summary)
# - zoomer_phones_100_processed_[timestamp].csv (Clean data export)
```

### **Expected Results:**
- **Data Quality**: ~95-100% validation rate
- **Processing Time**: ~2-3 minutes for 100 products
- **Insights Generated**: 10-15 key business insights
- **Visualizations**: 4 professional charts
- **Export Formats**: JSON, CSV, HTML reports

## ğŸ”§ Advanced Usage

### **Data Processing Only**
```bash
# Clean and validate existing data
python analyze_data.py process --input data_output/raw/zoomer_phones_50.json --output data_output/processed
```

### **Multiple File Analysis**
```bash
# Analyze multiple datasets together
python analyze_data.py analyze --input data_output/raw/zoomer_phones_50.json data_output/raw/zoomer_laptops_30.json --output data_output/combined_analysis
```

### **Custom Output Directory**
```bash
# Specify custom output location
python analyze_data.py pipeline --input data_output/raw/zoomer_laptops_100.json --output /custom/path/laptop_reports
```

### **Export Format Selection**
```bash
# Choose specific export formats
python analyze_data.py process --input data.json --formats json csv excel
```

## ğŸ“ˆ Analysis Features

### **Statistical Analysis**
- **Descriptive Statistics**: Mean, median, std deviation, quartiles
- **Distribution Analysis**: Skewness, kurtosis, normality tests
- **Price Intelligence**: Outlier detection, price segmentation
- **Market Analysis**: Brand share, category performance

### **Trend Analysis**
- **Time Patterns**: Daily, hourly, weekly trends
- **Price Trends**: Historical price movements
- **Volume Analysis**: Scraping volume patterns
- **Seasonal Patterns**: Monthly and seasonal trends

### **Data Quality**
- **Validation Rules**: Required fields, price ranges, categories
- **Quality Scoring**: 0-100 quality score per record
- **Data Cleaning**: Text normalization, date standardization
- **Feature Extraction**: Storage capacity, RAM, specifications

### **Visualizations**
- **Price Distribution**: Histograms and box plots
- **Category Analysis**: Bar charts and pie charts
- **Brand Analysis**: Market share and price positioning
- **Time Series**: Trend lines and pattern analysis

## ğŸ”§ Configuration

### **Scraping Settings** (`src/scrapers/zoomer_scraper/zoomer_scraper/settings.py`)
```python
# Performance settings
DOWNLOAD_DELAY = 1                    # Delay between requests
CONCURRENT_REQUESTS = 4               # Concurrent requests
CONCURRENT_REQUESTS_PER_DOMAIN = 3    # Per-domain concurrency

# AutoThrottle settings
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0
```

### **Analysis Settings**
- **Price Range Validation**: 0-50,000 GEL
- **Required Fields**: source, name, price, brand, category, createdat
- **Quality Thresholds**: Configurable scoring criteria
- **Export Formats**: JSON, CSV, Excel support

## ğŸ“Š Sample Output

### **Executive Summary Example:**
```
E-COMMERCE DATA ANALYSIS REPORT SUMMARY
==================================================

Generated: 2025-06-28 21:10:47
Total Records: 100
Date Range: 2025-06-28 to 2025-06-28

KEY INSIGHTS:
1. Average product price is 3,299 GEL
2. Apple dominates with 65.0% market share
3. Price distribution is heavily skewed towards lower prices
4. Peak scraping activity at 14:00-16:00
5. 98% data quality score achieved

Files Generated:
- JSON Report: ecommerce_analysis_report_20250628_211047.json
- HTML Dashboard: ecommerce_analysis_report_20250628_211047.html
- Visualizations: 4 PNG charts generated
- Clean Data: CSV and JSON exports available
```

### **Generated Files:**
- **ğŸ“Š `price_distribution.png`**: Price histograms and box plots
- **ğŸ“ˆ `category_analysis.png`**: Category breakdown charts
- **ğŸ¢ `brand_analysis.png`**: Brand market share analysis
- **â° `time_analysis.png`**: Time-based pattern analysis
- **ğŸ“„ `report.html`**: Interactive HTML dashboard
- **ğŸ’¾ `processed_data.csv`**: Clean, validated dataset

## ğŸ¯ Use Cases

### **Market Research**
- **Competitive Analysis**: Compare prices across brands
- **Market Trends**: Track pricing and availability patterns
- **Product Intelligence**: Identify popular features and specifications

### **Business Intelligence**
- **Pricing Strategy**: Understand market positioning
- **Inventory Planning**: Analyze product category performance
- **Market Share**: Track brand dominance and trends

### **Data Science**
- **Price Prediction**: Historical trend analysis
- **Anomaly Detection**: Identify unusual pricing patterns
- **Customer Insights**: Understand product preferences

## ğŸ› Troubleshooting

### **Common Issues**

**1. Import Errors:**
```bash
# Ensure all dependencies are installed
pip install -r requirements.txt
```

**2. Empty Results:**
```bash
# Check if data file exists and is valid JSON
python -m json.tool data_output/raw/zoomer_phones_10.json
```

**3. Permission Errors:**
```bash
# Ensure output directories exist
mkdir -p data_output/{raw,processed,reports}
```

**4. Rate Limiting:**
- Scraper automatically handles rate limiting
- Adjust `DOWNLOAD_DELAY` in settings if needed

### **Debug Mode**
```bash
# Enable verbose logging
python analyze_data.py analyze --input data.json --verbose
```

## ğŸ“ Data Schema

### **Raw Data Format:**
```json
{
  "source": "zoommer.ge",
  "name": "Apple iPhone 16 Pro Max 256GB",
  "price": 3299,
  "brand": "Apple",
  "category": "phones",
  "description": "Detailed product specifications...",
  "createdat": "2025-06-28T17:08:36.141170"
}
```

### **Processed Data Features:**
- **Quality Score**: Data quality assessment (0-100)
- **Extracted Features**: Storage capacity, RAM, processor info
- **Standardized Fields**: Cleaned text, normalized prices
- **Time Features**: Date, hour, weekday derived fields

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/new-feature`)
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ”— Links

- **Project Repository**: [GitHub Link]
- **Documentation**: `/docs` directory
- **Issue Tracker**: [GitHub Issues]
- **Wiki**: [Project Wiki]

## ğŸ† Project Status

**Version**: 1.0.0  
**Status**: âœ… Production Ready  
**Last Updated**: June 2025  
**Maintainer**: Development Team  

### **Compliance**
- âœ… University Project Requirements Met
- âœ… Industry Best Practices Applied
- âœ… Production-Grade Architecture
- âœ… Comprehensive Documentation

---

**Happy Scraping & Analyzing! ğŸš€ğŸ“Š**