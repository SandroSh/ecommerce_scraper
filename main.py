import argparse
import os
import sys
import logging
import glob
import traceback
import subprocess
from pathlib import Path
from scrapy.crawler import CrawlerProcess
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Optional

# Add src directory to Python path for imports
sys.path.insert(0, 'src')
from src.data.processors import DataProcessor, DataAggregator


class ErrorTracker:
    """Track and report errors during execution"""

    def __init__(self):
        self.errors = []
        self.warnings = []
        self.start_time = datetime.now()

    def log_error(self, component: str, error: Exception, context: str = ""):
        """Log an error with context"""
        error_info = {
            'timestamp': datetime.now(),
            'component': component,
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context,
            'traceback': traceback.format_exc()
        }
        self.errors.append(error_info)

    def log_warning(self, component: str, message: str, context: str = ""):
        """Log a warning with context"""
        warning_info = {
            'timestamp': datetime.now(),
            'component': component,
            'message': message,
            'context': context
        }
        self.warnings.append(warning_info)

    def generate_diagnostics_report(self, output_dir: str = "data_output/diagnostics"):
        """Generate comprehensive diagnostics report"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        execution_time = datetime.now() - self.start_time

        report = {
            'execution_summary': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_execution_time': str(execution_time),
                'total_errors': len(self.errors),
                'total_warnings': len(self.warnings)
            },
            'errors': self.errors,
            'warnings': self.warnings,
            'system_info': {
                'python_version': sys.version,
                'platform': sys.platform,
                'working_directory': os.getcwd()
            }
        }

        # Save detailed report as JSON
        report_file = f"{output_dir}/diagnostics_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            import json
            json.dump(report, f, indent=2, default=str)

        return report_file


class InteractiveExportMenu:
    """Interactive menu for data export options"""

    EXPORT_OPTIONS = {
        '1': (['json'], 'JSON only'),
        '2': (['csv'], 'CSV only'),
        '3': (['excel'], 'Excel only'),
        '4': (['json', 'csv'], 'JSON and CSV'),
        '5': (['json', 'excel'], 'JSON and Excel'),
        '6': (['csv', 'excel'], 'CSV and Excel'),
        '7': (['json', 'csv', 'excel'], 'All formats (JSON, CSV, Excel)')
    }

    @classmethod
    def display_menu(cls):
        """Display export options menu"""
        print("\n" + "=" * 50)
        print("üìÅ DATA EXPORT OPTIONS")
        print("=" * 50)
        for key, (formats, description) in cls.EXPORT_OPTIONS.items():
            print(f"{key}) {description}")
        print("=" * 50)

    @classmethod
    def get_user_choice(cls) -> List[str]:
        """Get user's export format choice"""
        while True:
            cls.display_menu()
            choice = input("\nSelect export format (1-7): ").strip()

            if choice in cls.EXPORT_OPTIONS:
                formats, description = cls.EXPORT_OPTIONS[choice]
                print(f"\n‚úÖ Selected: {description}")
                return formats
            else:
                print("\n‚ùå Invalid choice. Please select a number between 1-7.")


def configure_logging():
    """Configure logging with enhanced format"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/scraper_{timestamp}.log'),
            logging.StreamHandler()
        ]
    )


def run_zoomer_scraper(args, logger, error_tracker):
    """Run Zoomer scraper with Scrapy"""
    try:
        logger.info("üöÄ Starting Zoomer scraper...")

        from src.scrapers.zoomer_scraper.zoomer_scraper import settings as spider_settings
        from src.scrapers.zoomer_scraper.zoomer_scraper.spiders.zoomer_spider import ZoomerSpider

        settings = {
            'DOWNLOAD_DELAY': spider_settings.DOWNLOAD_DELAY,
            'CONCURRENT_REQUESTS': spider_settings.CONCURRENT_REQUESTS,
            'CONCURRENT_REQUESTS_PER_DOMAIN': spider_settings.CONCURRENT_REQUESTS_PER_DOMAIN,
            'AUTOTHROTTLE_ENABLED': spider_settings.AUTOTHROTTLE_ENABLED,
            'AUTOTHROTTLE_TARGET_CONCURRENCY': spider_settings.AUTOTHROTTLE_TARGET_CONCURRENCY,
            'AUTOTHROTTLE_START_DELAY': spider_settings.AUTOTHROTTLE_START_DELAY,
            'AUTOTHROTTLE_MAX_DELAY': spider_settings.AUTOTHROTTLE_MAX_DELAY,
            'USER_AGENT': spider_settings.USER_AGENT,
            'LOG_LEVEL': spider_settings.LOG_LEVEL,
            'ROBOTSTXT_OBEY': spider_settings.ROBOTSTXT_OBEY,
            'RANDOMIZE_DOWNLOAD_DELAY': spider_settings.RANDOMIZE_DOWNLOAD_DELAY,
            'DEFAULT_REQUEST_HEADERS': spider_settings.DEFAULT_REQUEST_HEADERS,
            'RETRY_ENABLED': spider_settings.RETRY_ENABLED,
            'RETRY_TIMES': spider_settings.RETRY_TIMES,
            'RETRY_HTTP_CODES': spider_settings.RETRY_HTTP_CODES,
            'LOG_FILE': f'logs/scrapy_zoomer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            'FEEDS': {
                f'data_output/raw/zoomer_{args.category}_{args.max_products}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json': {
                    'format': 'json',
                    'encoding': 'utf8',
                }
            }
        }

        process = CrawlerProcess(settings)
        process.crawl(ZoomerSpider, category=args.category, max_products=args.max_products)
        process.start()

        logger.info("‚úÖ Zoomer scraping completed")
        return True

    except Exception as e:
        error_tracker.log_error("ZoomerScraper", e, f"Category: {args.category}, Max products: {args.max_products}")
        logger.error(f"‚ùå Zoomer scraping failed: {e}")
        return False


def run_alta_scraper(args, logger, error_tracker):
    """Run Alta scraper with Selenium"""
    try:
        logger.info("üöÄ Starting Alta scraper...")

        sys.path.insert(0, 'src/scrapers/alta_scraper')
        from alta_selenium_scraper import AltaScraper

        scraper = AltaScraper(
            headless=True,
            max_products=args.max_products,
            debug=False
        )
        filepath = scraper.run(args.category)

        if filepath:
            logger.info(f"‚úÖ Alta scraping completed. Data saved to: {filepath}")
            return True
        else:
            error_tracker.log_warning("AltaScraper", "Scraping completed but no data was saved",
                                      f"Category: {args.category}")
            logger.warning("‚ö†Ô∏è Alta scraping completed but no data was saved")
            return False

    except ImportError as e:
        error_tracker.log_error("AltaScraper", e, "Import error - check if Alta scraper dependencies are installed")
        logger.error(f"‚ùå Failed to import Alta scraper: {e}")
        return False
    except Exception as e:
        error_tracker.log_error("AltaScraper", e, f"Category: {args.category}, Max products: {args.max_products}")
        logger.error(f"‚ùå Alta scraping failed: {e}")
        return False


def process_raw_data_combined(args, logger, error_tracker, export_formats: List[str]):
    """Process and combine cleaned valid data from all raw JSON files into one dataset."""
    try:
        logger.info("üîÑ Starting combined data processing...")

        processor = DataProcessor()
        output_dir = "data_output/processed"
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        raw_data_dir = "data_output/raw"
        raw_files = glob.glob(f"{raw_data_dir}/*.json")

        if not raw_files:
            error_tracker.log_warning("DataProcessor", "No raw data files found", f"Directory: {raw_data_dir}")
            logger.warning("No raw data files found in 'data_output/raw'")
            return False

        logger.info(f"Found {len(raw_files)} raw data files")

        cleaned_dfs = []
        total_valid = 0
        total_loaded = 0
        processing_errors = 0

        for file_path in raw_files:
            try:
                logger.info(f"üì• Loading file: {file_path}")
                df = processor.load_raw_data(file_path)
                total_loaded += len(df)

                if df.empty:
                    error_tracker.log_warning("DataProcessor", f"Empty file skipped", f"File: {file_path}")
                    logger.warning(f"Skipping empty file: {file_path}")
                    continue

                valid_df, report = processor.validate_data(df)
                cleaned_df = processor.clean_data(valid_df)

                cleaned_dfs.append(cleaned_df)
                total_valid += len(cleaned_df)

                logger.info(f"‚úÖ {file_path}: {len(cleaned_df)} valid records (from {len(df)})")

            except Exception as e:
                processing_errors += 1
                error_tracker.log_error("DataProcessor", e, f"Processing file: {file_path}")
                logger.error(f"‚ùå Error processing file {file_path}: {e}")

        # Combine all cleaned records
        if not cleaned_dfs:
            error_tracker.log_error("DataProcessor", Exception("No valid data found"),
                                    f"Processed {len(raw_files)} files, {processing_errors} errors")
            logger.error("‚ùå No valid data found in any file")
            return False

        combined_df = pd.concat(cleaned_dfs, ignore_index=True)
        logger.info(f"üìä Total valid combined records: {len(combined_df)} (from {total_loaded} scraped)")

        # Export to selected formats
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f"{output_dir}/all_sources_combined_{timestamp}"

        logger.info(f"üìÅ Exporting data in formats: {', '.join(export_formats)}")
        exported = processor.export_data(combined_df, output_path, formats=export_formats)

        logger.info("‚úÖ Combined export completed")
        for fmt, path in exported.items():
            logger.info(f"  üìÅ {fmt.upper()}: {path}")

        return True

    except Exception as e:
        error_tracker.log_error("DataProcessor", e, "Combined data processing")
        logger.error(f"‚ùå Data processing failed: {e}")
        return False


def run_automated_analysis(logger, error_tracker):
    """Automatically run data analysis after processing"""
    try:
        logger.info("üîç Starting automated data analysis...")

        # Check if analyze_data.py exists
        analysis_script = "analyze_data.py"
        if not os.path.exists(analysis_script):
            error_tracker.log_warning("AutoAnalysis", f"Analysis script not found: {analysis_script}",
                                      "Skipping automated analysis")
            logger.warning(f"‚ö†Ô∏è {analysis_script} not found. Skipping automated analysis.")
            return False

        # Run the analysis script
        result = subprocess.run([sys.executable, analysis_script],
                                capture_output=True, text=True, timeout=300)

        if result.returncode == 0:
            logger.info("‚úÖ Automated data analysis completed successfully")
            if result.stdout:
                logger.info("Analysis output:")
                for line in result.stdout.split('\n'):
                    if line.strip():
                        logger.info(f"  {line}")
            return True
        else:
            error_tracker.log_error("AutoAnalysis",
                                    Exception(f"Analysis script failed with return code {result.returncode}"),
                                    f"stderr: {result.stderr}")
            logger.error(f"‚ùå Analysis script failed: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        error_tracker.log_error("AutoAnalysis", Exception("Analysis script timeout"),
                                "Script took longer than 5 minutes")
        logger.error("‚ùå Analysis script timed out (5 minutes)")
        return False
    except Exception as e:
        error_tracker.log_error("AutoAnalysis", e, "Running automated analysis")
        logger.error(f"‚ùå Failed to run automated analysis: {e}")
        return False


def print_execution_summary(error_tracker, logger, scraping_success, processing_success, analysis_success):
    """Print comprehensive execution summary"""
    print("\n" + "=" * 80)
    print("üìä EXECUTION SUMMARY")
    print("=" * 80)

    execution_time = datetime.now() - error_tracker.start_time

    print(f"‚è±Ô∏è  Total execution time: {execution_time}")
    print(f"‚úÖ Scraping: {'SUCCESS' if scraping_success else 'FAILED'}")
    print(f"üîÑ Processing: {'SUCCESS' if processing_success else 'FAILED'}")
    print(f"üîç Analysis: {'SUCCESS' if analysis_success else 'FAILED/SKIPPED'}")

    if error_tracker.errors:
        print(f"\n‚ùå Errors encountered: {len(error_tracker.errors)}")
        for i, error in enumerate(error_tracker.errors[-3:], 1):  # Show last 3 errors
            print(f"   {i}. {error['component']}: {error['error_type']} - {error['error_message']}")
        if len(error_tracker.errors) > 3:
            print(f"   ... and {len(error_tracker.errors) - 3} more errors")

    if error_tracker.warnings:
        print(f"\n‚ö†Ô∏è  Warnings: {len(error_tracker.warnings)}")
        for i, warning in enumerate(error_tracker.warnings[-3:], 1):  # Show last 3 warnings
            print(f"   {i}. {warning['component']}: {warning['message']}")
        if len(error_tracker.warnings) > 3:
            print(f"   ... and {len(error_tracker.warnings) - 3} more warnings")

    print("=" * 80)


def main():
    configure_logging()
    logger = logging.getLogger(__name__)
    error_tracker = ErrorTracker()

    parser = argparse.ArgumentParser(description='Run E-commerce product scraper and data processor')
    parser.add_argument('--category', type=str, default='phones',
                        choices=['phones', 'fridges', 'laptops', 'tvs'],
                        help='Product category to scrape')
    parser.add_argument('--max_products', type=int, default=10,
                        help='Maximum number of products to scrape')
    parser.add_argument('--model_version', type=str, default='v1',
                        choices=['v1', 'v2', 'v3'],
                        help='Data processing model version')
    parser.add_argument('--scraper', type=str, default='both',
                        choices=['zoomer', 'alta', 'both'],
                        help='Which scraper to run')
    parser.add_argument('--process-only', action='store_true',
                        help='Skip scraping and only process existing raw data')
    parser.add_argument('--skip-processing', action='store_true',
                        help='Skip data processing after scraping')
    parser.add_argument('--skip-analysis', action='store_true',
                        help='Skip automated analysis after processing')
    parser.add_argument('--export-formats', type=str, nargs='+',
                        choices=['json', 'csv', 'excel'],
                        help='Export formats (bypasses interactive menu)')
    parser.add_argument('--generate-diagnostics', action='store_true',
                        help='Generate detailed diagnostics report')

    args = parser.parse_args()

    try:
        logger.info("üéØ Starting E-commerce Data Pipeline")
        logger.info("=" * 60)
        logger.info(f"  Category: {args.category}")
        logger.info(f"  Max products: {args.max_products}")
        logger.info(f"  Scraper: {args.scraper}")
        logger.info(f"  Process only: {args.process_only}")
        logger.info(f"  Skip processing: {args.skip_processing}")
        logger.info(f"  Skip analysis: {args.skip_analysis}")
        logger.info("=" * 60)

        scraping_success = True
        processing_success = True
        analysis_success = True

        # Scraping phase
        if not args.process_only:
            logger.info("üöÄ PHASE 1: DATA SCRAPING")
            scraping_results = []

            if args.scraper in ['zoomer', 'both']:
                result = run_zoomer_scraper(args, logger, error_tracker)
                scraping_results.append(result)

            if args.scraper in ['alta', 'both']:
                result = run_alta_scraper(args, logger, error_tracker)
                scraping_results.append(result)

            scraping_success = any(scraping_results) if scraping_results else False

            if not scraping_success:
                error_tracker.log_error("Pipeline", Exception("All scrapers failed"), "Scraping phase")
                logger.error("‚ùå All scrapers failed!")

        # Processing phase
        if not args.skip_processing:
            logger.info("\nüîÑ PHASE 2: DATA PROCESSING")

            # Get export formats
            if args.export_formats:
                export_formats = args.export_formats
                logger.info(f"Using command-line export formats: {export_formats}")
            else:
                export_formats = InteractiveExportMenu.get_user_choice()

            processing_success = process_raw_data_combined(args, logger, error_tracker, export_formats)
        else:
            logger.info("‚è≠Ô∏è  Skipping data processing phase")

        # Analysis phase
        if not args.skip_analysis and processing_success:
            logger.info("\nüîç PHASE 3: AUTOMATED ANALYSIS")
            analysis_success = run_automated_analysis(logger, error_tracker)
        else:
            if args.skip_analysis:
                logger.info("‚è≠Ô∏è  Skipping automated analysis phase")
            else:
                logger.info("‚è≠Ô∏è  Skipping analysis due to processing failure")
                analysis_success = False

        # Generate diagnostics report
        if args.generate_diagnostics or error_tracker.errors:
            logger.info("\nüìã GENERATING DIAGNOSTICS REPORT")
            diagnostics_file = error_tracker.generate_diagnostics_report()
            logger.info(f"üìÅ Diagnostics report saved: {diagnostics_file}")

        # Print final summary
        print_execution_summary(error_tracker, logger, scraping_success, processing_success, analysis_success)

        if scraping_success and processing_success:
            logger.info("\nüéâ Pipeline completed successfully!")
        else:
            logger.warning("\n‚ö†Ô∏è  Pipeline completed with some failures. Check logs for details.")

    except KeyboardInterrupt:
        error_tracker.log_error("Pipeline", Exception("User interrupted execution"), "KeyboardInterrupt")
        logger.error("\n‚ùå Execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        error_tracker.log_error("Pipeline", e, "Main execution")
        logger.error(f"\n‚ùå Unexpected error in main execution: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()